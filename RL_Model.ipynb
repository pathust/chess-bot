{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from evaluation.evaluation import Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_GAMES = 100\n",
    "INPUT_DIM = 774\n",
    "POLICY_DIM = 64 * 64\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "STOCKFISH_PATH = \"C:/Users/Windows/Downloads/stockfish-windows-x86-64-avx2/stockfish/stockfish.exe\"\n",
    "eval = Evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_to_index = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEN encoding function for the chess board\n",
    "def encode_fen(fen: str) -> torch.Tensor:\n",
    "    board = chess.Board(fen)\n",
    "    tensor = torch.zeros(774, dtype=torch.float)  # Increased tensor size to 774\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            idx = piece_to_index[piece.symbol()]\n",
    "            tensor[idx * 64 + square] = 1.0\n",
    "    offset = 768\n",
    "    tensor[offset] = 1.0 if board.turn == chess.WHITE else 0.0\n",
    "    offset += 1\n",
    "    tensor[offset] = 1.0 if board.has_kingside_castling_rights(chess.WHITE) else 0.0\n",
    "    tensor[offset + 1] = 1.0 if board.has_queenside_castling_rights(chess.WHITE) else 0.0\n",
    "    tensor[offset + 2] = 1.0 if board.has_kingside_castling_rights(chess.BLACK) else 0.0\n",
    "    tensor[offset + 3] = 1.0 if board.has_queenside_castling_rights(chess.BLACK) else 0.0\n",
    "    offset += 4\n",
    "    tensor[offset] = board.ep_square % 8 / 7.0 if board.ep_square else -1.0\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for Policy and Value Function\n",
    "class PolicyValueNet(nn.Module):\n",
    "    def __init__(self, input_dim, policy_output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.policy_head = nn.Linear(256, policy_output_dim)\n",
    "        self.value_head = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy_logits, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_to_index(move):\n",
    "    return move.from_square * 64 + move.to_square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_move(index):\n",
    "    return chess.Move(index // 64, index % 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(fen: str) -> float:\n",
    "    board = chess.Board(fen)\n",
    "    return eval.evaluate(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, move=None, parent=None):\n",
    "        self.move = move\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.N = 0  # Visit count\n",
    "        self.W = 0  # Win count\n",
    "        self.Q = 0  # Action value (Q-value)\n",
    "        self.P = 0  # Prior probability from the policy head\n",
    "\n",
    "    def is_fully_expanded(self, legal_moves):\n",
    "        return len(self.children) == len(legal_moves)\n",
    "\n",
    "    def best_child(self, c_param=1.0):\n",
    "        if len(self.children) == 0:\n",
    "            return None  # or raise an exception if no children are available\n",
    "        \n",
    "        best_child = None\n",
    "        best_value = -float('inf')\n",
    "        for child in self.children.values():\n",
    "            u = child.Q + c_param * child.P * math.sqrt(self.N) / (1 + child.N)\n",
    "            if u > best_value:\n",
    "                best_child = child\n",
    "                best_value = u\n",
    "        return best_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, model, num_simulations=1000, c_param=1.0):\n",
    "        self.model = model\n",
    "        self.num_simulations = num_simulations\n",
    "        self.c_param = c_param\n",
    "\n",
    "    def search(self, board, encode_fn):\n",
    "        random_move = random.choice(list(chess.Board().legal_moves))\n",
    "        board.push(random_move)\n",
    "        root = TreeNode(move = random_move)\n",
    "        root.P = self.get_policy_from_nn(board, encode_fn)\n",
    "        \n",
    "        # Ensure that root.move is valid before using it\n",
    "        if root.move is None:\n",
    "            print(\"Error: Root move is None!\")\n",
    "            return None  # Handle error appropriately\n",
    "        \n",
    "        for _ in range(self.num_simulations):\n",
    "            value = self.simulate(root, board)\n",
    "            self.backpropagate(root, value)\n",
    "        \n",
    "        best_move_node = root.best_child(self.c_param)\n",
    "        return best_move_node.move\n",
    "\n",
    "    def select(self, node):\n",
    "        while not node.is_fully_expanded():\n",
    "            node = node.best_child(self.c_param)\n",
    "        return node\n",
    "\n",
    "    def expand(self, node, board):\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        if not legal_moves:\n",
    "            return  # No legal moves to expand, do nothing\n",
    "        for move in legal_moves:\n",
    "            if move not in node.children:\n",
    "                new_node = TreeNode(move=move, parent=node)\n",
    "                new_node.P = self.get_policy_from_nn(board, move)\n",
    "                node.children[move] = new_node\n",
    "\n",
    "    def simulate(self, node, board):\n",
    "        move = node.move\n",
    "        if move is None:\n",
    "            print(\"Error: Invalid move!\")\n",
    "            return 0  # Or handle as needed, like returning a default value\n",
    "\n",
    "        board.push(move)\n",
    "        if board.is_game_over():\n",
    "            return evaluation(board.fen())\n",
    "        return 0\n",
    "\n",
    "    def backpropagate(self, node, value):\n",
    "        while node is not None:\n",
    "            node.N += 1\n",
    "            node.W += value\n",
    "            node.Q = node.W / node.N\n",
    "            node = node.parent\n",
    "\n",
    "    def get_policy_from_nn(self, board, encode_fn):\n",
    "        input_tensor = encode_fn(board.fen()).to(DEVICE).unsqueeze(0)\n",
    "        policy_logits, _ = self.model(input_tensor)\n",
    "        policy_probs = F.softmax(policy_logits, dim=1).detach().cpu().numpy().squeeze()\n",
    "        legal_moves = list(board.legal_moves)\n",
    "        legal_indices = [move_to_index(m) for m in legal_moves]\n",
    "        legal_probs = np.array([policy_probs[i] for i in legal_indices])\n",
    "        total_prob = legal_probs.sum()\n",
    "        if total_prob == 0:\n",
    "            legal_probs += 1e-8  # Avoid divide by zero\n",
    "            total_prob = legal_probs.sum()\n",
    "        return legal_probs / total_prob  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Move.from_uci('g1h3'), Move.from_uci('g1f3'), Move.from_uci('b1c3'), Move.from_uci('b1a3'), Move.from_uci('h2h3'), Move.from_uci('g2g3'), Move.from_uci('f2f3'), Move.from_uci('e2e3'), Move.from_uci('d2d3'), Move.from_uci('c2c3'), Move.from_uci('b2b3'), Move.from_uci('a2a3'), Move.from_uci('h2h4'), Move.from_uci('g2g4'), Move.from_uci('f2f4'), Move.from_uci('e2e4'), Move.from_uci('d2d4'), Move.from_uci('c2c4'), Move.from_uci('b2b4'), Move.from_uci('a2a4')]\n"
     ]
    }
   ],
   "source": [
    "board = chess.Board()\n",
    "print(list(board.legal_moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_with_mcts(model, mcts, encode_fn):\n",
    "    board = chess.Board()\n",
    "    game_data = []\n",
    "\n",
    "    while not board.is_game_over():\n",
    "        if board.turn == chess.WHITE:\n",
    "            move = mcts.search(board, encode_fn)\n",
    "        else:\n",
    "            move = chess.engine.SimpleEngine.popen_uci(STOCKFISH_PATH).play(board, chess.engine.Limit(time=0.1)).move\n",
    "\n",
    "        if move is None:\n",
    "            print(\"Warning: move is None.\")\n",
    "            break\n",
    "\n",
    "        game_data.append((board.fen(), move))\n",
    "        board.push(move)\n",
    "\n",
    "    return game_data, board.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_to_value(result, side='white'):\n",
    "    if result == '1-0': return 1 if side == 'white' else -1\n",
    "    if result == '0-1': return -1 if side == 'white' else 1\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, data, encode_fn):\n",
    "    model.train()\n",
    "    for fen, move, reward in data:\n",
    "        input_tensor = encode_fn(fen).to(DEVICE).unsqueeze(0)\n",
    "        move_index = move_to_index(move)\n",
    "        target_policy = torch.zeros(1, POLICY_DIM).to(DEVICE)\n",
    "        target_policy[0, move_index] = 1.0\n",
    "        target_value = torch.tensor([[reward]], dtype=torch.float).to(DEVICE)\n",
    "\n",
    "        policy_logits, value = model(input_tensor)\n",
    "        loss_policy = F.cross_entropy(policy_logits, target_policy)\n",
    "        loss_value = F.mse_loss(value, target_value)\n",
    "\n",
    "        loss = loss_policy + loss_value\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "\n",
    "# Initialize the board\n",
    "board = chess.Board()\n",
    "\n",
    "# Check for legal moves\n",
    "legal_moves = list(board.legal_moves)\n",
    "print(\"Legal moves:\", legal_moves)\n",
    "\n",
    "# Attempt to apply a move and check if it is legal\n",
    "move = chess.Move.from_uci('d2d4')\n",
    "if move in legal_moves:\n",
    "    board.push(move)\n",
    "else:\n",
    "    print(\"Illegal move:\", move)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_play_training_loop(evaluation_fn, encode_fn, num_games=NUM_GAMES):\n",
    "    model = PolicyValueNet(INPUT_DIM, POLICY_DIM).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    mcts = MCTS(model)\n",
    "\n",
    "    for game_num in range(num_games):\n",
    "        print(f\"[Game {game_num + 1}/{num_games}] Playing...\")\n",
    "        game_data, result = play_game_with_mcts(model, mcts, encode_fn)\n",
    "        reward = result_to_value(result)\n",
    "        training_data = [(fen, move, reward) for fen, move in game_data]\n",
    "        train_model(model, optimizer, training_data, encode_fn)\n",
    "        print(f\"Game result: {result} | Reward: {reward}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Game 1/100] Playing...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "push() expects move to be pseudo-legal, but got d2d4 in rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run self-play training loop\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m trained_model = \u001b[43mself_play_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_fen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_games\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mself_play_training_loop\u001b[39m\u001b[34m(evaluation_fn, encode_fn, num_games)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m game_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_games):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Game \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame_num\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_games\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Playing...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     game_data, result = \u001b[43mplay_game_with_mcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmcts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     reward = result_to_value(result)\n\u001b[32m     10\u001b[39m     training_data = [(fen, move, reward) \u001b[38;5;28;01mfor\u001b[39;00m fen, move \u001b[38;5;129;01min\u001b[39;00m game_data]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mplay_game_with_mcts\u001b[39m\u001b[34m(model, mcts, encode_fn)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m board.is_game_over():\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m board.turn == chess.WHITE:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         move = \u001b[43mmcts\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      9\u001b[39m         move = chess.engine.SimpleEngine.popen_uci(STOCKFISH_PATH).play(board, chess.engine.Limit(time=\u001b[32m0.1\u001b[39m)).move\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mMCTS.search\u001b[39m\u001b[34m(self, board, encode_fn)\u001b[39m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Handle error appropriately\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_simulations):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mself\u001b[39m.backpropagate(root, value)\n\u001b[32m     22\u001b[39m best_move_node = root.best_child(\u001b[38;5;28mself\u001b[39m.c_param)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mMCTS.simulate\u001b[39m\u001b[34m(self, node, board)\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mError: Invalid move!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Or handle as needed, like returning a default value\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mboard\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m board.is_game_over():\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluation(board.fen())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Windows\\Documents\\chess-bot\\.venv\\Lib\\site-packages\\chess\\__init__.py:2168\u001b[39m, in \u001b[36mBoard.push\u001b[39m\u001b[34m(self, move)\u001b[39m\n\u001b[32m   2166\u001b[39m promoted = \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m.promoted & from_bb)\n\u001b[32m   2167\u001b[39m piece_type = \u001b[38;5;28mself\u001b[39m._remove_piece_at(move.from_square)\n\u001b[32m-> \u001b[39m\u001b[32m2168\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m piece_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpush() expects move to be pseudo-legal, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmove\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.board_fen()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2169\u001b[39m capture_square = move.to_square\n\u001b[32m   2170\u001b[39m captured_piece_type = \u001b[38;5;28mself\u001b[39m.piece_type_at(capture_square)\n",
      "\u001b[31mAssertionError\u001b[39m: push() expects move to be pseudo-legal, but got d2d4 in rnbqkbnr/pppppppp/8/8/3P4/8/PPP1PPPP/RNBQKBNR"
     ]
    }
   ],
   "source": [
    "# Run self-play training loop\n",
    "trained_model = self_play_training_loop(evaluation, encode_fen, num_games=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(trained_model.state_dict(), \"self_trained_bot_with_mcts.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
